{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ptt_food_jieba.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TqkIJA-s3xms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snownlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUSoNbAL3ysK",
        "outputId": "a5135013-3416-43b9-e9f8-285f3c2532e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snownlp\n",
            "  Downloading snownlp-0.12.3.tar.gz (37.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 37.6 MB 1.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: snownlp\n",
            "  Building wheel for snownlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for snownlp: filename=snownlp-0.12.3-py3-none-any.whl size=37760963 sha256=24c973b232174a504fc5a6cee3eaaed71a931c6861d51220f92a7588cce9c689\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/7a/fe/a5747434679b22a95c93bcf9fa49a988f5d9be56366bdf6c79\n",
            "Successfully built snownlp\n",
            "Installing collected packages: snownlp\n",
            "Successfully installed snownlp-0.12.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.将csv文件中的文本逐行取出，存新的txt文件"
      ],
      "metadata": {
        "id": "JXywMdR8zGXn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZd3Bkvzxq7x",
        "outputId": "b1acedc1-a6cb-4c07-cc20-6d5e9e38e05e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "写入完成\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/ptt_food/ptt_food.csv', encoding='utf-8')\n",
        "# print(df.head())\n",
        "\n",
        "for text in df['body']:\n",
        "    # print(text)\n",
        "    if text is not None:\n",
        "        with open('/content/drive/MyDrive/ptt_food/ptt_body.txt', mode='a', encoding='utf-8') as file:\n",
        "            file.write(str(text))\n",
        "\n",
        "print('寫入完成')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.使用停用词获取最后的文本内容"
      ],
      "metadata": {
        "id": "wST9PKHhzFn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jieba\n",
        "\n",
        "jieba.load_userdict('/content/drive/MyDrive/ptt_food/user_dict.txt')\n",
        "# 创建停用词list\n",
        "def stopwordslist(filepath):\n",
        "    stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]\n",
        "    return stopwords\n",
        "\n",
        "# 對句子進行分词\n",
        "def seg_sentence(sentence):\n",
        "    sentence_seged = jieba.cut(sentence.strip())\n",
        "    stopwords = stopwordslist('/content/drive/MyDrive/ptt_food/stop_words.txt')  # 停用詞的路徑\n",
        "    outstr = ''\n",
        "    for word in sentence_seged:\n",
        "        if word not in stopwords:\n",
        "            if word != '\\t':\n",
        "                outstr += word\n",
        "                outstr += \" \"\n",
        "    return outstr\n",
        "\n",
        "inputs = open('/content/drive/MyDrive/ptt_food/ptt_body.txt', 'r', encoding='utf-8')\n",
        "outputs = open('/content/drive/MyDrive/ptt_food/ptt_body_outputs.txt', 'w', encoding='utf-8')\n",
        "for line in inputs:\n",
        "    line_seg = seg_sentence(line)  # 這裡的返回值是字符串\n",
        "    print(line_seg)\n",
        "    outputs.write(line_seg + '\\n')\n",
        "outputs.close()\n",
        "inputs.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60qgOWk0zSnP",
        "outputId": "698ddff0-c552-4157-bbeb-446a3e4ac503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1 jieba 分詞其他應用 - 關鍵詞提取"
      ],
      "metadata": {
        "id": "WP47rwuGGHZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jieba.analyse\n",
        "content = open('/content/drive/MyDrive/ptt_food/ptt_body_outputs.txt', 'r', encoding='utf-8')\n",
        "keywords = jieba.analyse.extract_tags(content, topK=20, withWeight=True, allowPOS=())\n",
        "# 訪問提取結果\n",
        "for item in keywords:\n",
        "    # 分別爲關鍵詞和相應的權重\n",
        "    print(item[0], item[1])\n",
        "\n",
        "# 同樣是四個參數，但allowPOS默認爲('ns', 'n', 'vn', 'v')\n",
        "# 即僅提取地名、名詞、動名詞、動詞\n",
        "# keywords = jieba.analyse.textrank(content, topK=20, withWeight=True, allowPOS=('ns', 'n', 'vn', 'v'))\n",
        "# # 訪問提取結果\n",
        "# for item in keywords:\n",
        "#     # 分別爲關鍵詞和相應的權重\n",
        "#     print(item[0], item[1])"
      ],
      "metadata": {
        "id": "oyNrMREGGCmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.制作词云图"
      ],
      "metadata": {
        "id": "jVrosap6zdYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import jieba\n",
        "import numpy\n",
        "import PIL.Image as Image\n",
        "\n",
        "def cut(text):\n",
        "    wordlist_jieba=jieba.cut(text)\n",
        "    space_wordlist=\" \".join(wordlist_jieba)\n",
        "    return space_wordlist\n",
        "with open(r\"/content/drive/MyDrive/ptt_food/ptt_body.txt\" ,encoding=\"utf-8\")as file:\n",
        "    text=file.read()\n",
        "    text=cut(text)\n",
        "    mask_pic=numpy.array(Image.open(r\"/content/drive/MyDrive/ptt_food/Taiwan.jpg\"))\n",
        "    wordcloud = WordCloud(font_path=r\"/content/drive/MyDrive/ptt_food/微軟正黑體-1.ttf\",\n",
        "    collocations=False,\n",
        "    max_words= 100,\n",
        "    min_font_size=10, \n",
        "    max_font_size=500,\n",
        "    mask=mask_pic).generate(text)\n",
        "    image=wordcloud.to_image()\n",
        "    # image.show()\n",
        "    wordcloud.to_file('/content/drive/MyDrive/ptt_food/词云图.png')  # 把词云保存下来"
      ],
      "metadata": {
        "id": "clf_YMdKzgIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.分词统计"
      ],
      "metadata": {
        "id": "f95_jm2dzmY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import jieba\n",
        "import jieba.analyse\n",
        "import json  # 把詞頻字典 json.dumps() 存成 json 以便下次用 json.loads() 讀取\n",
        "import xlwt  # 寫入Excel表的库\n",
        "\n",
        "# reload(sys)\n",
        "# sys.setdefaultencoding('utf-8')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    wbk = xlwt.Workbook(encoding='ascii')\n",
        "    sheet = wbk.add_sheet(\"wordCount\")  # Excel单元格名字\n",
        "    word_lst = []\n",
        "    key_list = []\n",
        "    for line in open('/content/drive/MyDrive/ptt_food/ptt_body.txt', encoding='utf-8'):  # 需要分词统计的原始目标文档\n",
        "\n",
        "        item = line.strip('\\n\\r').split('\\t')  # 制表格切分\n",
        "        print(item)\n",
        "        tags = jieba.analyse.extract_tags(item[0])  # jieba分词\n",
        "        for t in tags:\n",
        "            word_lst.append(t)\n",
        "\n",
        "    word_dict = {}\n",
        "    with open(\"分词结果.txt\", 'w') as wf2:  # 指定生成文件的名称\n",
        "\n",
        "        for item in word_lst:\n",
        "            if item not in word_dict:  # 统计數量\n",
        "                word_dict[item] = 1\n",
        "            else:\n",
        "                word_dict[item] += 1\n",
        "\n",
        "        orderList = list(word_dict.values())\n",
        "        orderList.sort(reverse=True)\n",
        "        # print orderList\n",
        "        for i in range(len(orderList)):\n",
        "            for key in word_dict:\n",
        "                if word_dict[key] == orderList[i]:\n",
        "                    wf2.write(key + ' ' + str(word_dict[key]) + '\\n')  # 寫入txt文檔\n",
        "                    key_list.append(key)\n",
        "                    word_dict[key] = 0\n",
        "    with open('word_dict.json', 'w') as fp:\n",
        "        json.dump(word_dict, fp)\n",
        "        \n",
        "    for i in range(len(key_list)):\n",
        "        sheet.write(i, 1, label=orderList[i])\n",
        "        sheet.write(i, 0, label=key_list[i])\n",
        "    wbk.save('/content/drive/MyDrive/ptt_food/wordCount_all_bodies.xls')  # 保存為 wordCount.xls文件\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qwWKyMeznvT",
        "outputId": "b6b4a0aa-cdf1-48f3-b505-b9bf5074a8f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.情感分析的统计值"
      ],
      "metadata": {
        "id": "UYI4mNvNzs6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from snownlp import SnowNLP\n",
        "\n",
        "# 積極/消極\n",
        "# print(s.sentiments)  # 0.9769551298267365  positive的概率\n",
        "\n",
        "\n",
        "def get_word():\n",
        "    with open(\"/content/drive/MyDrive/ptt_food/ptt_body.txt\", encoding='utf-8') as f:\n",
        "        line = f.readline()\n",
        "        print(line)\n",
        "        word_list = []\n",
        "        while line:\n",
        "            line = f.readline()\n",
        "            word_list.append(line.strip('\\r\\n'))\n",
        "        f.close()\n",
        "        return word_list\n",
        "\n",
        "\n",
        "def get_sentiment(word):\n",
        "    text = u'{}'.format(word)\n",
        "    try:\n",
        "        s = SnowNLP(text)\n",
        "        print(s.sentiments)\n",
        "    except ZeroDivisionError:\n",
        "        print(word, ' = ZeroDivisionError')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    words = get_word()\n",
        "    for word in words:\n",
        "        get_sentiment(word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7gVFTHVzvJL",
        "outputId": "d6183818-5a20-4698-ea01-ce3d4a4aa3f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MVw26_Vv3re2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}